{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,os.path\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-df87577a3d97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m# we store the documents inside a new folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mnew_title\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\Daniele\\fileAIR\\doc_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.tsv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mcsv_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[1;31m# we skip the first column which is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mrow1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creating a .tsv document for each review\n",
    "with open('Airbnb_Texas_Rentals.tsv', 'r', encoding = \"utf8\") as all_rev:\n",
    "    csv_reader = csv.reader(all_rev)\n",
    "    ind = -1\n",
    "    for row in all_rev:\n",
    "        ind += 1\n",
    "        if ind == 0:\n",
    "            continue\n",
    "        # skipping the empty lines\n",
    "        elif row == '\\n':\n",
    "            ind -= 1\n",
    "            # we manipulate 'ind' in order to have files named doc_i where i = 1,2,3,..\n",
    "        else:\n",
    "            # we store the documents inside a new folder\n",
    "            new_title = r'C:\\Users\\Daniele\\fileAIR\\doc_' + str(ind-1) + '.tsv'\n",
    "            csv_writer = csv.writer(open(new_title, 'w', encoding = \"utf8\"), delimiter ='\\t')\n",
    "            # we skip the first column which is \n",
    "            row1 = row.split(\"\\t\")[1:]\n",
    "            csv_writer.writerow(row1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess the files\n",
    "def preprocess(something):\n",
    "    something = something.lower()\n",
    "    # removing '\\n'\n",
    "    something = something.replace('\\\\n', ' ')\n",
    "    # removing punctuation\n",
    "    tokenizer = regexp_tokenize(something, \"[\\w'\\$]+\")\n",
    "    # filter the non stopwords\n",
    "    filtered = [w for w in tokenizer if not w in stopwords.words('english')]\n",
    "    ps = PorterStemmer()\n",
    "    # removing the stem\n",
    "    filtered = [ps.stem(word) for word in filtered]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "len_file = len([x for x in os.scandir(r'C:\\Users\\Daniele\\fileAIR')])\n",
    "print (len_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_set = set()\n",
    "docs_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len_file):\n",
    "    with open(r'C:\\Users\\Daniele\\fileAIR\\doc_' + str(i) + '.tsv', 'r', encoding = 'utf8') as csvfile:\n",
    "        file1 = csv.reader(csvfile, delimiter = '\\t')\n",
    "        columns = [i for i in file1]\n",
    "        description = columns[0][4]\n",
    "        description = preprocess(description)\n",
    "        title = columns[0][7]\n",
    "        title = preprocess(title)\n",
    "        tit_desc = title + description\n",
    "        vocabulary_set.update(tit_desc)\n",
    "        docs_list.append(set(tit_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {k:v for v, k in enumerate(vocabulary_set)}\n",
    "#vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary to the file\n",
    "voc_file = open(r'C:\\Users\\Daniele\\vocabulary.txt', 'w', encoding = \"utf8\")\n",
    "for term in vocabulary:\n",
    "    voc_file.write('{0}\\t{1}\\n'.format(term, vocabulary[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use defaultdict because it gives no key error\n",
    "from collections import defaultdict \n",
    "#we need t sort this value\n",
    "inv_indx = defaultdict(set)\n",
    "for idx, text in enumerate(docs_list):\n",
    "    for word in text:\n",
    "        id_word = vocabulary[word]\n",
    "        inv_indx[id_word].add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have a list as the value inside the dict\n",
    "for key in inv_indx:\n",
    "    value = inv_indx[key]\n",
    "    inv_indx[key] = list(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save inv_indx to the .txt file\n",
    "inv_file = open(r'C:\\Users\\Daniele\\inverted_indx.txt', 'w', encoding = \"utf8\")\n",
    "for id_term in inv_indx:\n",
    "    docks = inv_indx[id_term]\n",
    "    d = '\\t'.join(map(str, docks))\n",
    "    inv_file.write('{0}\\t{1}\\n'.format(id_term, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a beautiful house with garden and beach\n"
     ]
    }
   ],
   "source": [
    "query = str(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine(query, vocab, inv_indx):\n",
    "    doc_list = []\n",
    "    li = preprocess(query)\n",
    "    pos_list = []\n",
    "    for item in li:\n",
    "        if item not in vocabulary:\n",
    "            print('No documents found')\n",
    "            break\n",
    "        pos_id = vocabulary[item]\n",
    "        pos_list.append(pos_id)\n",
    "    result_list = []\n",
    "    for term_id in pos_list:\n",
    "        result_list.append(inv_indx[term_id])\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTheSomething(res_list):\n",
    "    test = {k:1 for k in res_list[0]}\n",
    "    for sublist_ind in range(1, len(res_list)):\n",
    "        for k in res_list[sublist_ind]:\n",
    "            try:\n",
    "                test[k] += 1\n",
    "            except:\n",
    "                test[k] = 1\n",
    "    return [doc_id for doc_id in test if test[doc_id]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = SearchEngine(query, vocabulary, inv_indx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 35, 207]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = findTheSomething(pos)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>City</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unique Location! Alamo Heights - Designer Insp...</td>\n",
       "      <td>Stylish, fully remodeled home in upscale NW – ...</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>https://www.airbnb.com/rooms/17481455?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monroe Manor</td>\n",
       "      <td>Come relax on the water in our beautiful beach...</td>\n",
       "      <td>Houston</td>\n",
       "      <td>https://www.airbnb.com/rooms/19273801?location...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MarBella House Near the Galveston Beach</td>\n",
       "      <td>Location Location Location - Large 3 bdrm hous...</td>\n",
       "      <td>Galveston</td>\n",
       "      <td>https://www.airbnb.com/rooms/7459305?location=...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Unique Location! Alamo Heights - Designer Insp...   \n",
       "1                                       Monroe Manor   \n",
       "2            MarBella House Near the Galveston Beach   \n",
       "\n",
       "                                         Description         City  \\\n",
       "0  Stylish, fully remodeled home in upscale NW – ...  San Antonio   \n",
       "1  Come relax on the water in our beautiful beach...      Houston   \n",
       "2  Location Location Location - Large 3 bdrm hous...    Galveston   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://www.airbnb.com/rooms/17481455?location...  \n",
       "1  https://www.airbnb.com/rooms/19273801?location...  \n",
       "2  https://www.airbnb.com/rooms/7459305?location=...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=[]\n",
    "for i in doc:\n",
    "    g.append(pd.read_csv(r'C:\\Users\\Daniele\\fileAIR\\doc_'+str(i)+'.tsv',sep='\\t'))\n",
    "g1=[[j for j in i] for i in g]\n",
    "cols = ['1', '2', 'City', '4','Description','6','7','Title','Url']\n",
    "df1 = pd.DataFrame(g1,columns=cols)[['Title','Description','City','Url']]\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
