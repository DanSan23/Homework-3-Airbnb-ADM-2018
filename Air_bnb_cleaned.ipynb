{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all paths used in notebook ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all_files = r'C:\\Users\\Asia\\Documents\\all_files_1'\n",
    "path_all_files_doc = r'C:\\Users\\Asia\\Documents\\all_files_1\\doc_'\n",
    "path_vocabulary = r'C:\\Users\\Asia\\Documents\\vocabulary.txt'\n",
    "path_inverted_indx = 'C:\\Users\\Asia\\Documents\\inverted_indx.txt'\n",
    "path_inv_indx_tfid = r'C:\\Users\\Asia\\Documents\\inverted_indx_tfid.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the .tsv documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Airbnb_Texas_Rentals.tsv', 'r', encoding = \"utf8\") as all_rev:\n",
    "    csv_reader = csv.reader(all_rev)\n",
    "    ind = -1\n",
    "    for row in all_rev:\n",
    "        ind += 1\n",
    "        if ind == 0:\n",
    "            continue\n",
    "        # skipping the empty lines\n",
    "        elif row == '\\n':\n",
    "            ind -= 1\n",
    "            # we manipulate 'ind' in order to have files named doc_i where i = 1,2,3,..\n",
    "        else:\n",
    "            # we store the documents inside a new folder\n",
    "            new_title = r'fileAIR\\doc_' + str(ind-1) + '.tsv'\n",
    "            csv_writer = csv.writer(open(new_title, 'w', encoding = \"utf8\"), delimiter ='\\t')\n",
    "            # we skip the first column\n",
    "            row1 = row.split(\"\\t\")[1:]\n",
    "            csv_writer.writerow(row1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **preprocess** function converts words in files.\n",
    "* removing '\\n'\n",
    "* removing punctuation\n",
    "* filter the non stopwords\n",
    "* removing the stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    # removing '\\n'\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    # removing punctuation\n",
    "    tokenizer = regexp_tokenize(text, \"[\\w'\\$]+\")\n",
    "    # filter the non stopwords\n",
    "    filtered = [w for w in tokenizer if not w in stopwords.words('english')]\n",
    "    ps = PorterStemmer()\n",
    "    # removing the stem\n",
    "    filtered = [ps.stem(word) for word in filtered]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_file = len([x for x in os.scandir(path_all_files)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:24.449997\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "vocabulary_set = set()\n",
    "docs_list = []\n",
    "\n",
    "for i in range(1, len_file):\n",
    "    with open(path_all_files_doc + str(i) + '.tsv', 'r', encoding = 'utf8') as csvfile:\n",
    "        file1 = csv.reader(csvfile, delimiter = '\\t')\n",
    "        columns = [i for i in file1]\n",
    "        # we want to focus only on description and title\n",
    "        description = columns[0][4]\n",
    "        description = preprocess(description)\n",
    "        title = columns[0][7]\n",
    "        title = preprocess(title)\n",
    "        tit_desc = title + description\n",
    "        # creating the vocabulary\n",
    "        vocabulary_set.update(tit_desc)\n",
    "        docs_list.append(set(tit_desc))\n",
    "\n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {k:v for v, k in enumerate(vocabulary_set)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving vocabulary to the file 'vocabulary.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_file = open(path_vocabulary, 'w', encoding = \"utf8\")\n",
    "for term in vocabulary:\n",
    "    voc_file.write('{0}\\t{1}\\n'.format(term, vocabulary[term]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "inv_indx = defaultdict(set)\n",
    "for idx, text_dict in enumerate(docs_list):\n",
    "    for word in text_dict:\n",
    "        id_word = vocabulary[word]\n",
    "        inv_indx[id_word].add(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we operated on sets as values in dictionary. Now we want to have a list as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in inv_indx.items():\n",
    "    inv_indx[key] = list(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving inv_indx to the .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_file = open(path_inverted_indx, 'w', encoding = \"utf8\")\n",
    "for id_term in inv_indx:\n",
    "    docks = inv_indx[id_term]\n",
    "    d = '\\t'.join(map(str, docks))\n",
    "    inv_file.write('{0}\\t{1}\\n'.format(id_term, d))\n",
    "inv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **findTheBestDocuments** is a function which is searching for documents with the all words from a query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTheBestDocuments(docs_list, pre_query):\n",
    "    test = {k:1 for k in docs_list[0]}\n",
    "    for sublist_ind in range(1, len(docs_list)):\n",
    "        for k in docs_list[sublist_ind]:\n",
    "            try:\n",
    "                test[k] += 1\n",
    "            except:\n",
    "                test[k] = 1\n",
    "    return [doc_id for doc_id in test if test[doc_id] == len(pre_query)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SearchEngine** is a function with the arguments:\n",
    "* query - The input text from user\n",
    "* vocab - dictionary (saved in 'vocabulary.txt' file)\n",
    "* inv_indx - inverted index dictionary (saved in 'inv_ind.txt' file)\n",
    "\n",
    "The **output** is a list of doc_id's for the best documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine(query, vocabulary, inv_indx):\n",
    "    pre_query = preprocess(query)\n",
    "    word_list = []\n",
    "    for item in pre_query:\n",
    "        if item not in vocabulary:\n",
    "            print('No documents found')\n",
    "            break\n",
    "        word_id = vocabulary[item]\n",
    "        word_list.append(word_id)\n",
    "    #word_list contains the id's of words according to vocabulary file\n",
    "    \n",
    "    result_list = []\n",
    "    for term_id in word_list:\n",
    "        result_list.append(inv_indx[term_id])\n",
    "    #result_list contains the id's of documents which contain at least one word from the query\n",
    "    best_docs = findTheBestDocuments(result_list, pre_query)\n",
    "    return best_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test of the SearchEngine:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room garden kitchen\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected documents:  [6139, 14591, 15558, 15642]\n"
     ]
    }
   ],
   "source": [
    "test = SearchEngine(query, vocabulary, inv_indx)\n",
    "print(\"Selected documents: \", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table with the whole information from selected files: #TODO: [Someone has to run it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_files = []\n",
    "for i in test:\n",
    "    docs_files.append(pd.read_csv(path_all_files_doc + str(i) + '.tsv', sep = '\\t'))\n",
    "docs_list = [[row for row in doc_i] for doc_i in docs_files]\n",
    "cols = ['1', '2', 'City', '4','Description','6','7','Title','Url']\n",
    "pd.DataFrame(docs_list, columns = cols)[['Title','Description','City','Url']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "global inv_indx_tfid\n",
    "inv_indx_tfid = {} #Inverted Index dictionary with the TFIDF scores\n",
    "\n",
    "def computeTFIDF(freq_dict, doc_id):\n",
    "    numWords = len(freq_dict)\n",
    "    for word in freq_dict.keys():\n",
    "        word_id = vocabulary[word]\n",
    "        try:\n",
    "            inv_indx_tfid[word_id].append((doc_id, round(float(freq_dict[word])/numWords, 3)))\n",
    "        except:\n",
    "            inv_indx_tfid[word_id] = [(doc_id, round(float(freq_dict[word])/numWords, 3))]\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **occurenceNum** function calculates the words frequencity in one document and calls **computeTFIDF** to compute the TFID score.\n",
    "\n",
    "ARGS:\n",
    "* **index** - the document id number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurenceNum(index):\n",
    "    with open(path_all_files_doc + str(index) + '.tsv', 'r', encoding = \"utf8\") as doc:\n",
    "        file = csv.reader(doc, delimiter = '\\t')\n",
    "        columns = [i for i in file]\n",
    "        description = columns[0][4]\n",
    "        description = preprocess(description)\n",
    "        title = columns[0][7]\n",
    "        title = preprocess(title)\n",
    "        tit_desc = title + description\n",
    "        freq_dict = {}\n",
    "        for w in tit_desc:\n",
    "            try:\n",
    "                freq_dict[w] += 1\n",
    "            except:\n",
    "                freq_dict[w] = 1\n",
    "        return computeTFIDF(freq_dict, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the **occurenceNum** function inside the loop, the dictionary 'inv_indx_tfid' is created at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:17.674112\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()        \n",
    "\n",
    "for file in range(1, len_file):\n",
    "    occurenceNum(file)\n",
    "    \n",
    "print(datetime.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving **the Inverted Index file with TFIDF score** (inverted_indx_tfid.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_file_score = open(path_inv_indx_tfid, 'w', encoding = \"utf8\")\n",
    "for id_term, docks in inv_indx_tfid.items():\n",
    "    d = '\\t'.join(map(str, docks))\n",
    "    inv_file_score.write('{0}\\t{1}\\n'.format(id_term, d))\n",
    "inv_file_score.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing **the Cosine Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(x, y):\n",
    "    print('x', x)\n",
    "    print('y', y)\n",
    "    dot_product = np.dot(x, y)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    return dot_product / (norm_x * norm_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **query_tfid** calculates the TFID score for words in a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tfid(prep_query_list):\n",
    "    len_list = len(prep_query_list) \n",
    "    return [prep_query_list.count(word)/len_list for word in prep_query_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **get_tfid** returns the TFID score.\n",
    "\n",
    "ARGS:\n",
    "* arg_list - the list which is a value for a given word in 'inv_indx_tfid' dictionary.\n",
    "* doc_id - the document id for which function returns the tfid score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfid(arg_list, doc_id):\n",
    "    for tuple_ in arg_list:\n",
    "        if tuple_[0] == doc_id:\n",
    "            return tuple_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SearchEngine_new** is a function with the arguments:\n",
    "\n",
    "* query - The input text from user\n",
    "* vocabulary - dictionary (saved in 'vocabulary.txt' file)\n",
    "* inv_indx - inverted index dictionary (saved in 'inv_ind.txt' file)\n",
    "* inv_indx_tfid  - inverted index with the tfid score dictionary (saved in 'inv_ind_tfid.txt' file)\n",
    "\n",
    "The output is a list of (doc_ids, Cosine Similarity) for the best documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine_new(query, vocabulary, inv_indx, inv_indx_tfid):\n",
    "    query_list = preprocess(query)\n",
    "    word_list = []\n",
    "    for item in query_list:\n",
    "        if item not in vocabulary:\n",
    "            print('No documents found')\n",
    "            break\n",
    "        word_id = vocabulary[item]\n",
    "        word_list.append(word_id)\n",
    "        \n",
    "    result_list = []\n",
    "    for term_id in word_list:\n",
    "        result_list.append(inv_indx[term_id])\n",
    "    selected_docs = findTheBestDocuments(result_list, query_list)\n",
    "    \n",
    "    # Calculating the Cosine Similarities\n",
    "    cos_sim_list = []\n",
    "    for doc_id in selected_docs:\n",
    "        \n",
    "        # Creating the TFID vector for a document\n",
    "        tfid_vector = []\n",
    "        \n",
    "        for word in word_list: \n",
    "            g = get_tfid(inv_indx_tfid[word], doc_id)\n",
    "            tfid_vector.append(g)    \n",
    "        #tfid_vector = [(test, doc_id) for word in pos_list]\n",
    "        \n",
    "        cos_sim_list.append(cosine_sim(query_tfid(word_list), tfid_vector))\n",
    "    return cos_sim_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test of the SearchEngine_new:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "private bedroom\n"
     ]
    }
   ],
   "source": [
    "query = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [0.5, 0.5]\n",
      "y [None, None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-5955a993e615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSearchEngine_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_indx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_indx_tfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-85-ebdebab00b9a>\u001b[0m in \u001b[0;36mSearchEngine_new\u001b[1;34m(query, vocabulary, inv_indx, inv_indx_tfid)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#tfid_vector = [(test, doc_id) for word in pos_list]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mcos_sim_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcosine_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_tfid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfid_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcos_sim_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-d1eeba1aa1c7>\u001b[0m in \u001b[0;36mcosine_sim\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdot_product\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mnorm_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnorm_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "SearchEngine_new(query, vocabulary, inv_indx, inv_indx_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
